{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOquxqUDMChpVqv0HvlK0QG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah00ee/federated-learning-tutorial/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0A7xn9vqVN2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ff65f81-c219-4f53-95bf-1f268dddacbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syft==0.2.9\n",
            "  Downloading syft-0.2.9-py3-none-any.whl (433 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 235 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 245 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 286 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 327 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 348 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 389 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 399 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 430 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 433 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting importlib-resources~=1.5.0\n",
            "  Downloading importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting requests~=2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.4)\n",
            "Collecting syft-proto~=0.5.2\n",
            "  Downloading syft_proto-0.5.3-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.3)\n",
            "Requirement already satisfied: requests-toolbelt==0.9.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.9.1)\n",
            "Collecting RestrictedPython~=5.0\n",
            "  Downloading RestrictedPython-5.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting lz4~=3.0.2\n",
            "  Downloading lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 23.4 MB/s \n",
            "\u001b[?25hCollecting numpy~=1.18.1\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 10.4 MB/s \n",
            "\u001b[?25hCollecting aiortc==0.9.28\n",
            "  Downloading aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 45.4 MB/s \n",
            "\u001b[?25hCollecting websocket-client~=0.57.0\n",
            "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting torch~=1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.5 kB/s \n",
            "\u001b[?25hCollecting tornado==4.5.3\n",
            "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
            "\u001b[K     |████████████████████████████████| 484 kB 45.1 MB/s \n",
            "\u001b[?25hCollecting tblib~=1.6.0\n",
            "  Downloading tblib-1.6.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting openmined.threepio==0.2.0\n",
            "  Downloading openmined.threepio-0.2.0.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting websockets~=8.1.0\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n",
            "Collecting phe~=1.4.0\n",
            "  Downloading phe-1.4.0.tar.gz (35 kB)\n",
            "Collecting psutil==5.7.0\n",
            "  Downloading psutil-5.7.0.tar.gz (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 41.7 MB/s \n",
            "\u001b[?25hCollecting torchvision~=0.5.0\n",
            "  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 42.0 MB/s \n",
            "\u001b[?25hCollecting shaloop==0.2.1-alpha.11\n",
            "  Downloading shaloop-0.2.1_alpha.11-py3-none-manylinux1_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.1)\n",
            "Collecting notebook==5.7.8\n",
            "  Downloading notebook-5.7.8-py2.py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.4)\n",
            "Collecting pyee>=6.0.0\n",
            "  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\n",
            "Collecting pylibsrtp>=0.5.6\n",
            "  Downloading pylibsrtp-0.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (1.15.0)\n",
            "Collecting cryptography>=2.2\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 31.3 MB/s \n",
            "\u001b[?25hCollecting crc32c\n",
            "  Downloading crc32c-2.2.post0-cp37-cp37m-manylinux2010_x86_64.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting av<9.0.0,>=8.0.0\n",
            "  Downloading av-8.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.1 MB 226 kB/s \n",
            "\u001b[?25hCollecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading aioice-0.6.18-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (22.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.13.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.13.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (1.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (2.11.3)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.3.5)\n",
            "Requirement already satisfied: pycparser>=2 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (2.21)\n",
            "Collecting netifaces\n",
            "  Downloading netifaces-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (7.1.2)\n",
            "Collecting Werkzeug<2.0,>=0.15\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting python-socketio>=4.3.0\n",
            "  Downloading python_socketio-5.5.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook==5.7.8->syft==0.2.9) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pyee>=6.0.0->aiortc==0.9.28->syft==0.2.9) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (1.15.0)\n",
            "Collecting bidict>=0.21.0\n",
            "  Downloading bidict-0.21.4-py3-none-any.whl (36 kB)\n",
            "Collecting python-engineio>=4.3.0\n",
            "  Downloading python_engineio-4.3.1-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2021.10.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (1.24.3)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.5.2->syft==0.2.9) (3.19.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft==0.2.9) (0.7.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook==5.7.8->syft==0.2.9) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.2.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.6.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook==5.7.8->syft==0.2.9) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook==5.7.8->syft==0.2.9) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook==5.7.8->syft==0.2.9) (0.18.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (21.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.1)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (2.4.7)\n",
            "Building wheels for collected packages: openmined.threepio, psutil, tornado, phe\n",
            "  Building wheel for openmined.threepio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openmined.threepio: filename=openmined.threepio-0.2.0-py3-none-any.whl size=80094 sha256=ab74b89e39fa182468760b13e0aa07cddf3a98d0d3a1ee5d613ada3cf53cb55d\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/3d/ce/4ca4386006e622cb87d5116e5e65026ec021d3cf906a9b3d5d\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=276502 sha256=a102a1a7d957da9e741fca8a950099117601bd42a4cd54f8e7fc0efce65e773d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/e7/50/aee9cc966163d74430f13f208171dee22f11efa4a4a826661c\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434054 sha256=1f50e84e6bf42be31a7bfd187e119fe987ab2314ef9687ada7d13034b6b36266\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/45/43/36ec7a893e16c1212a6b1505ded0a2d73cf8e863a0227c8e04\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=7b3326e805dc709e302a437f104c211ac1b52988f52612109cd13f57d8c78eb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ac/9b/b07a04fe6bb1418ab4ee06d6652757aef848b80363c4dac507\n",
            "Successfully built openmined.threepio psutil tornado phe\n",
            "Installing collected packages: importlib-resources, tornado, Werkzeug, python-engineio, netifaces, idna, bidict, torch, requests, python-socketio, pylibsrtp, pyee, numpy, cryptography, crc32c, av, aioice, websockets, websocket-client, torchvision, tblib, syft-proto, shaloop, RestrictedPython, psutil, phe, openmined.threepio, notebook, lz4, flask-socketio, aiortc, syft\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.4.0\n",
            "    Uninstalling importlib-resources-5.4.0:\n",
            "      Successfully uninstalled importlib-resources-5.4.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 2.0.2\n",
            "    Uninstalling Werkzeug-2.0.2:\n",
            "      Successfully uninstalled Werkzeug-2.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.26.0\n",
            "    Uninstalling requests-2.26.0:\n",
            "      Successfully uninstalled requests-2.26.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.4\n",
            "    Uninstalling numpy-1.21.4:\n",
            "      Successfully uninstalled numpy-1.21.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: tblib\n",
            "    Found existing installation: tblib 1.7.0\n",
            "    Uninstalling tblib-1.7.0:\n",
            "      Successfully uninstalled tblib-1.7.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Attempting uninstall: syft\n",
            "    Found existing installation: syft 0.6.0\n",
            "    Uninstalling syft-0.6.0:\n",
            "      Successfully uninstalled syft-0.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.0+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.1 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.3.0; python_version >= \"3.0\", but you have notebook 5.7.8 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 4.5.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires tornado>=5.1, but you have tornado 4.5.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed RestrictedPython-5.2 Werkzeug-1.0.1 aioice-0.6.18 aiortc-0.9.28 av-8.1.0 bidict-0.21.4 crc32c-2.2.post0 cryptography-36.0.1 flask-socketio-4.2.1 idna-2.8 importlib-resources-1.5.0 lz4-3.0.2 netifaces-0.11.0 notebook-5.7.8 numpy-1.18.5 openmined.threepio-0.2.0 phe-1.4.0 psutil-5.7.0 pyee-9.0.4 pylibsrtp-0.7.1 python-engineio-4.3.1 python-socketio-5.5.2 requests-2.22.0 shaloop-0.2.1a11 syft-0.2.9 syft-proto-0.5.3 tblib-1.6.0 torch-1.4.0 torchvision-0.5.0 tornado-4.5.3 websocket-client-0.57.0 websockets-8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# This code is from https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KyubQq8aOIIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)"
      ],
      "metadata": {
        "id": "L4VQfStUAvWt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the VirtualWorkers\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  # define remote worker alice"
      ],
      "metadata": {
        "id": "DiTxU7vWA3Kq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "metadata": {
        "id": "tmofPfkQCtbw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and sending to workers\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])), batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "w6ZPVdr7BRz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8ee0a6-5d02-4c0e-a9fe-9cee995acf53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:The following options are not supported: num_workers: 1, pin_memory: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN specification\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "dv-I_DmJDwUz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the train and test functions\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader):     # a distributed dataset\n",
        "        model.send(data.location)   # send the model to the right location\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get()   # get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True)   # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "WaWaFYi7EYng"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the training\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)   # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "metadata": {
        "id": "ovNdoa2SILFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45d8ea4-c770-45c0-e574-d154d55a2a52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.311097\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.279700\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.266318\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.214711\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.167513\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.109019\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.089679\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 2.024196\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.832399\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.634013\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.387239\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 1.220130\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 1.058031\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.805720\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.795623\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.646950\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.485977\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.536385\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.554034\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.571430\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.658330\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.309597\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.468680\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.506902\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.388688\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.463429\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.421278\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.397830\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.447360\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.464311\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.632469\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.372508\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.582308\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.261494\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.446080\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.209324\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.444877\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.329410\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.390766\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.371865\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.235134\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.325399\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.271206\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.294662\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.209634\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.460765\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.357095\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.300625\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.336256\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.369925\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.106105\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.296394\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.241518\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.269017\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.221714\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.241855\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.191012\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.224156\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.283369\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.131289\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.176130\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.433533\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.102560\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.315797\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.189122\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.166003\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.255172\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.253567\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.345840\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.161702\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.297887\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.176295\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.093394\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.348200\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.211849\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.181639\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.248444\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.325325\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.089004\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.165504\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.332048\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.189098\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.126181\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.193675\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.134890\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.120423\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.257832\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.195607\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.063555\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.082501\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.207281\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.094832\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.159939\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.144636\n",
            "\n",
            "Test set: Average loss: 0.1622, Accuracy: 9519/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.134107\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.165990\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.155989\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.136553\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.240706\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.361294\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.088586\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.094983\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.143462\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.250294\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.292058\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.202781\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.183352\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.113853\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.184368\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.076051\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.088152\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.161384\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.241927\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.098564\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.132026\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.136122\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.292319\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.368245\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.152245\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.219245\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.329976\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.082938\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.063093\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.100602\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.183267\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.229046\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.146577\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.067166\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.096225\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.115555\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.053233\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.044833\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.074159\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.165180\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.085182\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.103629\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.121103\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.123769\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.123337\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.140723\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.189886\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.237652\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.120069\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.070409\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.164120\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.067102\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.101770\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.066751\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.154018\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.121233\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.071274\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.096632\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.045308\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.163938\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.095631\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.079713\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.066474\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.084860\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.136559\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.060653\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.126843\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.174394\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.331615\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.047146\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.035713\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.138401\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.087362\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.098717\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.138005\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.097510\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.144508\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.106201\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.076703\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.071299\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.117892\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.143265\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.081776\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.057288\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.300062\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.064656\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.087295\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.030914\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.044813\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.162332\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.111218\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.115276\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.076451\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.033474\n",
            "\n",
            "Test set: Average loss: 0.1055, Accuracy: 9702/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.047140\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.092778\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.163416\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.125178\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.086273\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.046104\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.118655\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.040060\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.040728\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.170909\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.130426\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.142735\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.033725\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.041198\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.055820\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.061588\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.061702\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.117244\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.156229\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.136353\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.029468\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.085492\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.226730\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.112832\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.094256\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.045028\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.095740\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.054611\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.035359\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.039773\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.118949\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.196924\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.101014\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.141132\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.179357\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.095934\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.192782\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.084149\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.129453\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.050120\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.068575\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.123221\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.203206\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.166822\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.097890\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.047140\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.124541\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.029779\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.078532\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.071738\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.114066\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.034859\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.035175\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.165243\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.062541\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.078862\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.079724\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.047193\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.175552\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.074570\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.054469\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.043990\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.025685\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.073006\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.145018\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.099818\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.092702\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.143828\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.059353\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.109458\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.064229\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.171236\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.041076\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.010455\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.042637\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.022747\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.055841\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.050514\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.138892\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.110794\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.096753\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.007585\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.134004\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.118675\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.070897\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.036922\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.037168\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.058343\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.131580\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.036945\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.121811\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.057699\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.071144\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.036836\n",
            "\n",
            "Test set: Average loss: 0.0684, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.050475\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.099578\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.058149\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.081218\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.102345\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.137359\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.077688\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.013467\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.039119\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.081044\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.065272\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.147423\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.144924\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.263933\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.013434\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.142108\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.041044\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.071754\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.079214\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.032460\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.104668\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.036374\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.089740\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.044017\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.069302\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.041260\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.016507\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.100614\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.081016\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.035937\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.009109\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.013218\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.017653\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.069670\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.062415\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.049365\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.120226\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.016231\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.093678\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.026215\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.059160\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.171459\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.010581\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.034633\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.068988\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.048298\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.009808\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.114374\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.033837\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.042032\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.026767\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.047625\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.081482\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.081552\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.074483\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.093991\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.031233\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.035262\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.057636\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.106782\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.057847\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.043885\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.082178\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.020093\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.079275\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.049025\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.038135\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.094854\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.043002\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.094764\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.017420\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.035911\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.025487\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.031827\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.017213\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.057782\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.034481\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.045000\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.027330\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.035543\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.063187\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.106928\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.042947\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.070823\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.077632\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.081147\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.069777\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.013485\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.072833\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.185111\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.085207\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.069468\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.020585\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.053208\n",
            "\n",
            "Test set: Average loss: 0.0579, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.026355\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.035552\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.081658\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.039762\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.117557\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.053296\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.051231\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.021368\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.112628\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.027192\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.014854\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.124790\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.054333\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.075609\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.010438\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.006238\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.067017\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.072557\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.097634\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.056058\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.063545\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.007079\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.011902\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.120568\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.027757\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.051335\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.155254\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.047750\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.112646\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.211765\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.071053\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.016535\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.011599\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.036531\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.021853\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.027883\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.009137\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.031669\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.016527\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.025381\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.019734\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.035020\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.075648\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.025788\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.056108\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.020845\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.081481\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.025847\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.015107\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.080460\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.119444\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.135945\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.096918\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.006110\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.081622\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.056870\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.048156\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.045554\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.063810\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.089737\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.013757\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.053849\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.146517\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.063527\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.019680\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.077321\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.076727\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.150734\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.047614\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.130418\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.027660\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.163756\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.040571\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.005914\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.058087\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.007770\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.025142\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.024233\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.048767\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.007916\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.036561\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.023212\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.061618\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.037975\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.007729\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.050047\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.114773\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.095412\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.036295\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.333832\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.063069\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.063937\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.049588\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.125632\n",
            "\n",
            "Test set: Average loss: 0.0469, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.009242\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.059658\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.100365\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.018558\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.029590\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.089834\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.020670\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.023852\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.031264\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.019852\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.029695\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.033834\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.033388\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.037651\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.020447\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.069845\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.019327\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.066969\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.004924\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.029999\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.017413\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.108065\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.065796\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.104381\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.023792\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.043910\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.009580\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.051601\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.096323\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.055415\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.053999\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.070674\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.088729\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.011106\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.062600\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.038204\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.026695\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.047392\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.055944\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.144562\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.021673\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.026049\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.020080\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.022628\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.056284\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.018283\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.072348\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.063683\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.030128\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.028458\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.016217\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.094789\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.037447\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.067143\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.016032\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.038970\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.051219\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.007627\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.034332\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.032878\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.032382\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.007211\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.061322\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.094119\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.024308\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.010188\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.054665\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.025418\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.075110\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.007088\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.012363\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.029360\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.031000\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.029906\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.010364\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.013395\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.055336\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.232680\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.109745\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.011578\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.059567\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.122166\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.113011\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.030036\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.109826\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.124089\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.054945\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.063383\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.010429\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.106884\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.012298\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.046144\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.014552\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.009814\n",
            "\n",
            "Test set: Average loss: 0.0586, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.150474\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.013130\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.018920\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.012697\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.054039\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.064681\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.015582\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.050704\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.050975\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.024792\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.023707\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.010710\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.065271\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.037764\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.011854\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.065749\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.028414\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.026096\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.028235\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.042044\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.009253\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.036006\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.103263\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.018932\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.036496\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.029525\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.034968\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.024362\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.014458\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.042779\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.084230\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.064696\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.139688\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.009328\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.043774\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.013473\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.044266\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.064094\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.007497\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.027146\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.009866\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.010624\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.032473\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.035324\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.037137\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.026152\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.048244\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.016027\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.015736\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.005182\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.088190\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.022821\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.029415\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.012795\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.172272\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.069227\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.033554\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.082106\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.064532\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.079811\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.003022\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.073595\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.062960\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.023160\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.026339\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.062601\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.012003\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.031659\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.093241\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.030494\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.042147\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.086866\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.022622\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.040493\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.037647\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.054828\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.040664\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.005546\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.080374\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.045751\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.022099\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.014043\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.060875\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.001742\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.073708\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.055454\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.030667\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.145815\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.021354\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.017923\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.047799\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.084742\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.018922\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.032546\n",
            "\n",
            "Test set: Average loss: 0.0432, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.015459\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.028649\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.036987\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.012868\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.066737\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.017506\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.008877\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.009973\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.010579\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.038424\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.026933\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.100730\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.051372\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.033248\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.059003\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.066502\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.098347\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.005165\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.016394\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.030195\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.026139\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.048065\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.046983\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.026057\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.015069\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.004921\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.037352\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.039065\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.054640\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.067227\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.038639\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.013091\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.021436\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.015513\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.043333\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.019926\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.033686\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.082981\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.016912\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.010675\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.015600\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.001576\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.010893\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.029485\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.034518\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.039784\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.020306\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.040760\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.017847\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.028478\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.121397\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.022385\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.041640\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.027882\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.094157\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.048599\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.024242\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.004127\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.005132\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.016758\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.024266\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.079485\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.014910\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.121454\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.011711\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.116257\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.036996\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.017417\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.018629\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.014537\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.007550\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.036975\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.017528\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.068111\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.051856\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.015221\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.043566\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.036431\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.093604\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.036860\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.016078\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.032432\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.013628\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.010109\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.013398\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.007975\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.012464\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.088186\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.020396\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.008817\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.019142\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.041360\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.016152\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.104649\n",
            "\n",
            "Test set: Average loss: 0.0406, Accuracy: 9871/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.016467\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.107003\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.043850\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.047693\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.050651\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.047255\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.016501\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.003204\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.039886\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.040548\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.006946\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.008007\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.031622\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.014107\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.026323\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.007599\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.008273\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.030134\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.030971\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.013371\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.008040\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.022548\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.012772\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.021140\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.029847\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.040928\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.042347\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.079231\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.010533\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.038252\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.115034\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.002678\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.075944\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.020463\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.017458\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.103938\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.027062\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.005743\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.013717\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.012505\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.019544\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.182175\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.023628\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.027917\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.007807\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.041198\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.017345\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.016196\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.004035\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.011036\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.058092\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.032174\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.017269\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.031297\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.154375\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.009498\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.016792\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.007402\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.004998\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.011780\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.015133\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.010392\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.014969\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.026658\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.012459\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.016556\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.028932\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.113159\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.007498\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.039559\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.034011\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.080567\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.018769\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.141922\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.042125\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.119082\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.064822\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.020087\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.057474\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.007930\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.025758\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.049244\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.073802\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.081059\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.040913\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.016423\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.040496\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.005488\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.022067\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.007793\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.015483\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.012040\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.052594\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.006905\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.019773\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.042740\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.017530\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.014186\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.018675\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.039815\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.006875\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.038878\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.014898\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.012408\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.011937\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.007018\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.138279\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.097908\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.007382\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.023896\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.018385\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.020525\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.014418\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.011397\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.030309\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.031259\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.004559\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.030149\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.013711\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.016271\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.026109\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.013666\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.064879\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.009154\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.038865\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.002632\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.067719\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.009454\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.006827\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.018361\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.015254\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.013343\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.017838\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.021631\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.006718\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.008571\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.008759\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.005437\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.020412\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.103813\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.029532\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.003668\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.027727\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.045575\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.083155\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.009631\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.056275\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.012896\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.070990\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.076391\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.057303\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.007600\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.031158\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.128662\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.040483\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.013463\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.024914\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.004798\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.037240\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.041339\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.003524\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.071583\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.011829\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.024726\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.036344\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.061015\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.038566\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.016518\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.022322\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.004738\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.132044\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.013567\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.004217\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.093076\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.005453\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.004534\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.037393\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.055817\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.055076\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.018941\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.029946\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.011299\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.343178\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.018891\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.005840\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.026085\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.006084\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.023019\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9888/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}